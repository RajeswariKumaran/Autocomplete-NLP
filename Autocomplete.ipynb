{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Autocomplete.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca6jCnJCqT3n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvAgI_5qqT3o"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLV9vDk4wcN7"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from sklearn.metrics.pairwise import pairwise_distances"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-wlrBBwce0"
      },
      "source": [
        "def splitDataFrameList(df,target_column,separator):\n",
        "    \n",
        "    ''' \n",
        "    source: https://gist.github.com/jlln/338b4b0b55bd6984f883 modified to keep punctuation\n",
        "    df = dataframe to split,\n",
        "    target_column = the column containing the values to split\n",
        "    separator = the symbol used to perform the split\n",
        "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
        "    The values in the other columns are duplicated across the newly divided rows.\n",
        "    '''\n",
        "    def split_text(line, separator):\n",
        "        splited_line =  [e+d for e in line.split(separator) if e]\n",
        "        return splited_line\n",
        "    \n",
        "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
        "        split_row = row[target_column].split(separator)\n",
        "        for s in split_row:\n",
        "            new_row = row.to_dict()\n",
        "            new_row[target_column] = s\n",
        "            row_accumulator.append(new_row)\n",
        "    new_rows = []\n",
        "    df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\n",
        "    new_df = pd.DataFrame(new_rows)\n",
        "    return new_df\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LQEOVXrwcmF"
      },
      "source": [
        "class Autocompleter:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def import_json(self, json_filename):\n",
        "        print(\"load json file...\")\n",
        "        df = load_df(json_filename)\n",
        "        return df\n",
        "        \n",
        "    def process_data(self, new_df):\n",
        "\n",
        "        \n",
        "        print(\"split sentenses on punctuation...\")\n",
        "        for sep in ['. ',', ','? ', '! ', '; ']:\n",
        "            new_df = splitDataFrameList(new_df, 'Text', sep)\n",
        "            \n",
        "        print(\"Text Cleaning using simple regex...\")\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: \" \".join(x.split()))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.strip(\".\"))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: \" \".join(x.split()))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' i ',' I '))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' ?','?'))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' !','!'))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' .','.'))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x.replace('OK','Ok'))\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x[0].upper()+x[1:])\n",
        "        new_df['Text']=new_df['Text'].apply(lambda x: x+\"?\" if re.search(r'^(Wh|How).+([^?])$',x) else x)\n",
        "        \n",
        "        print(\"calculate nb words of sentenses...\")\n",
        "        new_df['nb_words'] = new_df['Text'].apply(lambda x: len(str(x).split(' ')))\n",
        "        new_df = new_df[new_df['nb_words']>2]\n",
        "        \n",
        "        print(\"count occurence of sentenses...\")\n",
        "        new_df['Counts'] = new_df.groupby(['Text'])['Text'].transform('count')\n",
        "        \n",
        "        print(\"remove duplicates (keep last)...\")\n",
        "        new_df = new_df.drop_duplicates(subset=['Text'], keep='last')\n",
        "        \n",
        "        new_df = new_df.reset_index(drop=True)\n",
        "        print(new_df.shape)  \n",
        "        \n",
        "        return new_df\n",
        "    \n",
        "    def calc_matrice(self, df):\n",
        "        # define tfidf parameter in order to count/vectorize the description vector and then normalize it.\n",
        "        model_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 5), min_df=0)\n",
        "        tfidf_matrice = model_tf.fit_transform(df['Text'])\n",
        "        print(\"tfidf_matrice \", tfidf_matrice.shape)\n",
        "        return model_tf, tfidf_matrice\n",
        "\n",
        "    def generate_completions(self, prefix_string, data, model_tf, tfidf_matrice):\n",
        "        \n",
        "        prefix_string = str(prefix_string)\n",
        "        new_df = data.reset_index(drop=True)\n",
        "        weights = new_df['Counts'].apply(lambda x: 1+ np.log1p(x)).values\n",
        "\n",
        "        # tranform the string using the tfidf model\n",
        "        tfidf_matrice_spelling = model_tf.transform([prefix_string])\n",
        "        # calculate cosine_matrix\n",
        "        cosine_similarite = linear_kernel(tfidf_matrice, tfidf_matrice_spelling)\n",
        "        \n",
        "        #sort by order of similarity from 1 to 0:\n",
        "        similarity_scores = list(enumerate(cosine_similarite))\n",
        "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "        similarity_scores = similarity_scores[0:10]\n",
        "\n",
        "        similarity_scores = [i for i in similarity_scores]\n",
        "        similarity_indices = [i[0] for i in similarity_scores]\n",
        "\n",
        "        #add weight to the potential results that had high frequency in orig data\n",
        "        for i in range(len(similarity_scores)):\n",
        "            similarity_scores[i][1][0]=similarity_scores[i][1][0]*weights[similarity_indices][i]\n",
        "\n",
        "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "        similarity_scores = similarity_scores[0:3]\n",
        "        similarity_indices_w = [i[0] for i in similarity_scores]\n",
        "        \n",
        "        return new_df.loc[similarity_indices_w]['Text'].tolist()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhDzLfmhwcqT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzYv7IVqT3p"
      },
      "source": [
        "# Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Y5WJoyqT3p"
      },
      "source": [
        "#import autocompleter \n",
        "autocompl = Autocompleter()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofjcil2TqT3q",
        "outputId": "5997663e-1df1-45f5-d43f-316acd62ae1f"
      },
      "source": [
        "df = autocompl.import_json(\"sample_conversations.json\")\n",
        "df.shape, df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load json file...\n",
            "(22264, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22264, 3), Index(['IsFromCustomer', 'Text', 'index'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CIewF_6rSwn"
      },
      "source": [
        "df = pd.read_csv(\"g02-federalspending.txt\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEThb69VqT3s"
      },
      "source": [
        "The file contains 22K conversations between a customer and a representative.\n",
        "For the purpose of this project, we are only interested in completing the threads of the representative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "vmzHj--BqT3s",
        "outputId": "3c169553-f80f-4b8b-9a2e-339f1f6da79e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Given</th>\n",
              "      <th>When</th>\n",
              "      <th>Then</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a UI designer</td>\n",
              "      <td>I want to redesign the Resources page</td>\n",
              "      <td>so that it matches the new Broker design styles.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As a UI designer</td>\n",
              "      <td>I want to report to the Agencies about user t...</td>\n",
              "      <td>so that they are aware of their contributions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a UI designer</td>\n",
              "      <td>I want to move on to round 2 of DABS or FABS ...</td>\n",
              "      <td>so that I can get approvals from leadership.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As a UI designer</td>\n",
              "      <td>I want to move on to round 2 of Homepage edits</td>\n",
              "      <td>so that I can get approvals from leadership.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As a UI designer</td>\n",
              "      <td>I want to move on to round 3 of the Help page...</td>\n",
              "      <td>so that I can get approvals from leadership.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Given  ...                                               Then\n",
              "0  As a UI designer  ...   so that it matches the new Broker design styles.\n",
              "1  As a UI designer  ...   so that they are aware of their contributions...\n",
              "2  As a UI designer  ...       so that I can get approvals from leadership.\n",
              "3  As a UI designer  ...       so that I can get approvals from leadership.\n",
              "4  As a UI designer  ...       so that I can get approvals from leadership.\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz3u40NkqT3t"
      },
      "source": [
        "# Data Selection and Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fXQXhbUqT3u"
      },
      "source": [
        "The data is going to separate the threads from the customer and the representative, separate the sentenses based on the punctuation (we will keep the punctuation), the final text will be cleaned up with some light regex and only the sentense larger than 1 word will be kept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOtbqGMyqT3u"
      },
      "source": [
        "Finally, since the representative has the tendency to ask the same question over and over again, the autocomplete is extremely useful by suggesting a complete sentense. In our case, we will count the number of occurence of the same sentense so we can use it as a feature later on and delete the duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzpiIjQxqT3v"
      },
      "source": [
        "#new_df = autocompl.process_data(df)\n",
        "#new_df.shape, new_df.columns\n",
        "new_df=df"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sx-L9fbqT3v"
      },
      "source": [
        "# Model and TFIDF matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6TjM09mqT3w"
      },
      "source": [
        "A matrice of similarity is calculated based on the frequency of all the words in the data using tfidfvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "JHkAHIWvqT3w",
        "outputId": "51e8a6cd-6165-465b-c4c9-98c964f2bcbf"
      },
      "source": [
        "model_tf, tfidf_matrice = autocompl.calc_matrice(new_df)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-788fd48d4ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautocompl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_matrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-1a694193aba5>\u001b[0m in \u001b[0;36mcalc_matrice\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# define tfidf parameter in order to count/vectorize the description vector and then normalize it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmodel_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtfidf_matrice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_matrice \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PSceEhHqT3x"
      },
      "source": [
        "# Ranking Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXuyPquPsOuP"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xDAMA2tqT3x"
      },
      "source": [
        "Finally, the autocomplete is calculating the similarity between the sentense in the data and the prefix of the sentense written by the representative. As a weight feature, we chose to reorder using the frequency of the most common similar sentense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xjc7LnlqT3y"
      },
      "source": [
        "examples of auto completions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya3PopPxqT3y",
        "outputId": "81c1b182-93ea-40d9-e44a-df2ef71870da"
      },
      "source": [
        "prefix = 'What is your'\n",
        "\n",
        "print(prefix,\"    \\n \")\n",
        "\n",
        "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is your     \n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is your account number?',\n",
              " 'What is your order number?',\n",
              " 'What is your phone number?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWmhcfejqT3z",
        "outputId": "e32c98ac-82fd-4ea9-bc88-632491c5abdc"
      },
      "source": [
        "prefix = 'How can'\n",
        "print(prefix,\"     \")\n",
        "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How can      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How can I help you?',\n",
              " 'How can I help you today?',\n",
              " 'Ok lets see how I can help']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhQPCl4aqT30",
        "outputId": "323f7881-9540-4988-9909-48465283a93d"
      },
      "source": [
        "prefix = 'Let me'\n",
        "print(prefix,\"     \")\n",
        "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let me      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let me investigate', 'Let me assist you', 'Let me look']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErSrSGKbqT30",
        "outputId": "5ba12915-9e44-4b5e-d0ae-618cfee6fefa"
      },
      "source": [
        "prefix = 'when was'\n",
        "print(prefix,\"     \")\n",
        "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "when was      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When was the last time you changed your password?',\n",
              " 'When was your flight scheduled for?',\n",
              " 'When was the last time you tried?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_C9RZY_qT31"
      },
      "source": [
        "Now, without any uppercase and just with the important words..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdMXxqIqqT32",
        "outputId": "2c8e2d17-24c5-4d80-be27-b6091e81e180"
      },
      "source": [
        "prefix = 'when time password'\n",
        "print(prefix,\"     \")\n",
        "autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "when time password      \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When was the last time you changed your password?',\n",
              " 'When you select you password?',\n",
              " 'Take your time']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUFv4ReGqT32"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlh-dz-nqT32"
      },
      "source": [
        "# Online Sources for this project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "honkp6BOqT33"
      },
      "source": [
        "# https://gist.github.com/jlln/338b4b0b55bd6984f883 modified to keep punctuation\n",
        "# kaggle google store competition for json read\n",
        "# https://www.kaggle.com/hamishdickson/weighted-word-autocomplete-using-star-wars-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfP96i24qT33"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyNlk4-qqT33"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3fV_Fe2qT34"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}